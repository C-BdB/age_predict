{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io, transform\n",
    "class UTKFaceDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset_path, transform = None):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.transform = transform\n",
    "        images = []\n",
    "        genders = []\n",
    "        ages = []\n",
    "        for i in os.listdir(self.dataset_path)[0:8000]:\n",
    "            split = i.split('_')\n",
    "            ages.append(int(split[0]))\n",
    "            genders.append(int(split[1]))\n",
    "            images.append(os.path.join(self.dataset_path, i))\n",
    "        self.df = pd.DataFrame({\"Image\": images, \"Genders\": genders, \"Ages\": ages})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = io.imread(self.df.iloc[index][\"Image\"])\n",
    "        label = self.df.iloc[index][\"Genders\"]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((64,64),\n",
    "                          antialias = True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = UTKFaceDataset('../input/UTKFace',transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "indices = [i for i in range(len(dataset))]\n",
    "train_val_indices, test_indices = train_test_split(indices, test_size = 0.4, random_state = 31)\n",
    "train_indices, val_indices = train_test_split(train_val_indices, test_size = 0.3, random_state = 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(torch.utils.data.Subset(dataset,train_indices),\n",
    "                          batch_size=batch_size, shuffle = True)\n",
    "val_loader = DataLoader(torch.utils.data.Subset(dataset,val_indices),\n",
    "                        batch_size=batch_size, shuffle = False)\n",
    "test_loader = DataLoader(torch.utils.data.Subset(dataset,test_indices),\n",
    "                         batch_size=batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_images(dataset, N):\n",
    "    images = []\n",
    "    labels = []\n",
    "    image_index = np.random.choice([i for i in range(len(dataset))],\n",
    "                                    size = N,\n",
    "                                    replace = False)\n",
    "    for i in image_index:\n",
    "      images.append(dataset[i][0])\n",
    "      labels.append(dataset[i][1])\n",
    "    return torch.stack(images), labels\n",
    "\n",
    "def plot_images(images, nrow = 5):\n",
    "    greed = torchvision.utils.make_grid(\n",
    "        tensor = images,\n",
    "        nrow = nrow\n",
    "    )\n",
    "    plt.imshow(greed.cpu().permute(1, 2, 0))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"no. of images : {len(dataset)}\")\n",
    "print(f\"Size of images : {dataset[0][0].size()}\")\n",
    "images, labels = get_random_images(dataset, 25)\n",
    "plot_images(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DenseLayer, self).__init__()\n",
    "        self.Dense_Layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels,\n",
    "                      out_channels,\n",
    "                      kernel_size = 3,\n",
    "                      stride = 1,\n",
    "                      padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(out_channels) \n",
    "        )\n",
    "\n",
    "    def forward(self,x):        \n",
    "        out = torch.cat([x, self.Dense_Layer(x)], dim = 1)\n",
    "        return out\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, channels):    \n",
    "        super(DenseBlock, self).__init__()\n",
    "        self.Dense_Layers = nn.Sequential(\n",
    "            DenseLayer(channels * 1 , channels),\n",
    "            DenseLayer(channels * 2 , channels),\n",
    "            DenseLayer(channels * 3 , channels)\n",
    "        )\n",
    "        self.Conv_1x1 = nn.Conv2d(channels * 4, channels, \n",
    "                                  kernel_size = 1, \n",
    "                                  padding = 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.Dense_Layers(x)\n",
    "        out = self.Conv_1x1(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ConvolutionalBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "\n",
    "        super(ConvolutionalBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels,\n",
    "                              out_channels,\n",
    "                              kernel_size = 3,\n",
    "                              stride = 1,\n",
    "                              padding = 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batchnorm(x)\n",
    "        return x\n",
    "\n",
    "class TransposeConvolutionalBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(TransposeConvolutionalBlock, self).__init__()\n",
    "        self.conv = nn.ConvTranspose2d(in_channels,\n",
    "                                       out_channels,\n",
    "                                       kernel_size = 3,\n",
    "                                       stride = 2,\n",
    "                                       output_padding = 1,\n",
    "                                       padding = 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batchnorm(x)\n",
    "        return x\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        # Khushal :\n",
    "        # 3 x 64 x 64\n",
    "        # 32 x 32 x 32\n",
    "        # 64 x 16 x 16\n",
    "        # 128 x 8 x 8\n",
    "\n",
    "        self.encoder_conv = nn.Sequential(\n",
    "            ConvolutionalBlock(3, 32),\n",
    "            nn.MaxPool2d(kernel_size = 2),\n",
    "            DenseBlock(32),\n",
    "            ConvolutionalBlock(32, 64),\n",
    "            nn.MaxPool2d(kernel_size = 2),\n",
    "            DenseBlock(64),\n",
    "            ConvolutionalBlock(64, 128),\n",
    "            nn.MaxPool2d(kernel_size = 2)\n",
    "        )\n",
    "\n",
    "        # Khushal :\n",
    "        # 128 x 8 x 8\n",
    "        # 64 x 16 x 16\n",
    "        # 32 x 32 x 32\n",
    "        # 3 x 64 x 64\n",
    "\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "            TransposeConvolutionalBlock(128, 64),\n",
    "            TransposeConvolutionalBlock(64, 32),\n",
    "            nn.ConvTranspose2d(32, 3,\n",
    "                               kernel_size = 3,\n",
    "                               stride = 2,\n",
    "                               output_padding = 1,\n",
    "                               padding = 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder_conv(x)\n",
    "        x = self.decoder_conv(x)\n",
    "        return x\n",
    "\n",
    "model_ae = Autoencoder().to(device)\n",
    "print(model_ae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for i, (images, gender_labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            images, gender_labels = images.to(device), gender_labels.to(device)\n",
    "            reconstruction = model(images)\n",
    "            loss = criterion(reconstruction, images)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item()\n",
    "        epoch_train_loss = running_train_loss / len(train_loader)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "          for i, (images, gender_labels) in enumerate(val_loader):\n",
    "                images, gender_labels = images.to(device), gender_labels.to(device)\n",
    "                reconstruction = model(images)\n",
    "                loss = criterion(reconstruction, images)\n",
    "                running_val_loss += loss.item()\n",
    "        epoch_val_loss = running_val_loss / len(val_loader)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}')\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model_ae.parameters(), lr = 0.001)\n",
    "train_model(model_ae, train_loader, val_loader, criterion, optimizer, num_epochs = 20)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
